---
title: "DADA2 Pipeline"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
author: "Briana K. Whitaker"
date: "`r Sys.Date()`"
---
\fontsize{9}{10}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=3.5, fig.height=3,
                      warning=FALSE, message=FALSE)
```
---

* Run using `r version[['version.string']] `.

# Objective
This document reports on the **bioinformatics analysis** of the NC Microbiome Landscapes Project, performed in the summer of 2019. 


# 0) Load Packages, set path to data
```{r, echo=FALSE, results='hide', include=FALSE} 
x<-c("BiocManager", "dada2", "ShortRead", "Biostrings", "seqinr", "tidyverse", "ggplot2", "phyloseq")
lapply(x, require, character.only = TRUE)

#Load functions
source("./code/fxn_blastr-bw.R")
source("./code/blastn_code.R")

#add 'not in' function
`%nin%` = Negate(`%in%`)

#set seed
set.seed(392)

# set ggplot2 theme
theme_set(theme_bw(base_size=16)) 
theme_update(panel.grid.major=element_line(0), panel.grid.minor=element_line(0))

# set path for zipped and deindexed fastq files
path <- "./RawSq"
#list.files(path)
# 486 files, 239 samples + 3 neg controls + 1 pos control

#make a list of matched sample names
fnFs <- sort(list.files(path, pattern = "R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "R2_001.fastq.gz", full.names = TRUE))
# check lengths
#length(fnFs); length(fnRs)  #243 files each


get.Sample.ID <- function(fname) strsplit(basename(fname), "_")[[1]][1]  
ids.dat <- as.data.frame(cbind("num" = unname(sapply(fnFs, get.Sample.ID)),
                               fnFs, fnRs))
Sample.ID <- ids.dat$num
#tableOfSampleIDs <- sort(table(Sample.ID))
```


```{r, results='hide', echo=FALSE, include = FALSE}
# loop to count the number of seqs originally
#i = 1 #useful for checking
#fwdSeqs <- list()
#revSeqs <- list()
#for (i in 1:length(fnFs)) {
#  fwdSeqs[[i]] <- length(sapply(fnFs[i], getSequences))
#  revSeqs[[i]] <- length(sapply(fnRs[i], getSequences))
#}
#identical(c(unlist(fwdSeqs)),c(unlist(revSeqs))) #TRUE

#SeqsOrig.df <- data.frame(SampleID = c(basename(fnFs)) , 
#           OrigSeqsFwd = c(unlist(fwdSeqs)),  OrigSeqsRev = c(unlist(revSeqs)))
#rownames(SeqsOrig.df) <- SeqsOrig.df$SampleID
#SeqsOrig.df <- SeqsOrig.df[,-1]

#write.csv(SeqsOrig.df, "./intermediate/MBLand_TrackSequences_PriorFiltering.csv")
```


# 1) Initial Filter Step
```{r, results='hide'}
# filter out reads with ambiguous bases (N) only
# Put N-filterd files in filtN/ subdirectory
fnFs.filtN <- file.path(path, "filtN-", basename(fnFs)) 
fnRs.filtN <- file.path(path, "filtN-", basename(fnRs))

#filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)
```

```{r, results='hide', echo=FALSE}
#identify primers used, including ambiguous bases
FWD <- "CTTGGTCATTTAGAGGAAGTAA"       #ITS1F
REV <- "GCTGCGTTCTTCATCGATGC"         #ITS2

#check that we have the right orientation of both primers
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  #Biostrings needs DNAString objects
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
# return orientations
FWD.orients <- allOrients(FWD);FWD.orients
REV.orients <- allOrients(REV);REV.orients

#count no. times primers appear (and orientations), for 1 file only as representative
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

# read this table as -- the column headers indictaing the direction of the primer
# (i.e., forward direction of either the FWD or REV primer)
# and the rownames indicating the combo of primer (FWD/REV) and read type (Forward/Reverse)
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[97]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[97]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[97]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[97]]))
## checked this for non-SWCH and SWCH sample, to make sure PNA didn't mess it up
#100E             Forward Complement Reverse RevComp
#FWD.ForwardReads    7424          0       0       0
#FWD.ReverseReads       0          0       0    3731
#REV.ForwardReads       0          0       0    4482
#REV.ReverseReads   15928          0       0       0
#181             Forward Complement Reverse RevComp
#FWD.ForwardReads   12087          0       0       0
#FWD.ReverseReads       0          0       0    7595
#REV.ForwardReads       0          0       0    9333
#REV.ReverseReads   18846          0       0       0
```

# 2) Remove Primers
```{r, results='hide', echo=FALSE}
cutadapt <- "/Users/brianawhitaker/Library/Python/3.7/bin/cutadapt" #change to location on your machine
system2(cutadapt, args = "--version") #v.1.18

path.cut <- file.path(path, "cutadapt-")
#if (!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))


#define reverse complements of each primer (FOR LATER)
FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
```
```{r, results='hide'}
# add flags for cutadapt command
R1.flags <- paste("-g", FWD, "-a", REV.RC) #-g for 5' end, -a for 3' end
R2.flags <- paste("-G", REV, "-A", FWD.RC) 

# Run Cutadapt
#for (i in seq_along(fnFs)) {
#  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, "-m", 50, "-e", 0.1,
#              "-o", fnFs.cut[i], "-p", fnRs.cut[i],
#                    fnFs.filtN[i],     fnRs.filtN[i] ))   }
```

```{r, results='hide', echo=FALSE}
#sanity check, see if primers were removed from 1st sample
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[97]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[97]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[97]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[97]]))
#100E              Forward Complement Reverse RevComp
#FWD.ForwardReads       0          0       0       0
#FWD.ReverseReads       0          0       0       0
#REV.ForwardReads       0          0       0       0
#REV.ReverseReads       0          0       0       0
#181              Forward Complement Reverse RevComp
#FWD.ForwardReads       0          0       0       0
#FWD.ReverseReads       0          0       0       0
#REV.ForwardReads       0          0       0       0
#REV.ReverseReads       0          0       0       0


#get filenames of cutadapt-ed files
cutFs <- sort(list.files(path.cut, pattern = "R1_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R2_001.fastq.gz", full.names = TRUE))

#Sample.ID
```

### Inspect Quality Plots

```{r, warning=FALSE, message=FALSE, results='hide', echo=FALSE, fig.height=4, fig.width =4}
#inspect read quality plots
#pdf("figures/MBLand-SequenceQuality.pdf", 
#     width=16/2.54, height=16/2.54)
plotQualityProfile(cutFs[151:154]) + ggtitle("Wheat Fwd")
#plotQualityProfile(cutFs[1:4]) + ggtitle("Corn Fwd")
#plotQualityProfile(cutFs[58:61]) + ggtitle("Soy Fwd") 
#plotQualityProfile(cutFs[111:114]) + ggtitle("Swch Fwd") 
```

-

```{r, warning=FALSE, message=FALSE, results='hide', echo=FALSE, fig.height=4, fig.width =4}
plotQualityProfile(cutRs[151:154]) + ggtitle("Wheat Rev")
#plotQualityProfile(cutRs[1:4]) + ggtitle("Corn Rev")
#plotQualityProfile(cutRs[58:61]) + ggtitle("Soy Rev") 
#plotQualityProfile(cutRs[111:114]) + ggtitle("Swch Rev") 
#dev.off()
```


# 3) Filter and Trim

```{r, results='hide', echo = FALSE}
#set filenames for creating filtered files from cutadapt-ed files
filtFs <- file.path(path, "filtered-final", basename(cutFs))
filtRs <- file.path(path, "filtered-final", basename(cutRs))

```

*Results of tests of maxEE parameter in second filtering:* 
* Tested various maxEE at 2,2; 2,4; 4,6; 7,9
* 2,4 improves read counts, small benefit to ASV recovery
```{r, results='hide'}
#perform second filtering, keep maxN=0   #8min
#out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, 
#                     maxEE = c(2, 4), 
#    truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)
#save(out, file="./intermediate/MBLand_FilterOut.RData")  
load("./intermediate/MBLand_FilterOut.RData")
```

### Inspect the No. of reads, before and after 2nd filtering step
```{r}
head(out)
```


```{r, results='hide', echo=FALSE}
#write.csv(out, "./intermediate/MBLand-SecondFilteringStep.csv")
#out <- read.csv("./intermediate/MBLand-SecondFilteringStep.csv", row.names=1)
```


# 4) Learn Errors, Dereplicate, & Denoise

### Learn Errors
```{r, results='hide', echo=FALSE}
# to continue with pipeline, ignoring samples that don't pass the filter
not.lost <- out[,"reads.out"] > 0  
length(not.lost); dim(out)[1]  #no samples reduced to 0 reads
filtFs <- filtFs[not.lost]
filtRs <- filtRs[not.lost]

```


```{r, results='hide'}
#The DADA2 algorithm makes use of a parametric error model (err),
#errF <- learnErrors(filtFs, multithread=TRUE) #used 47 samples to learn
#errR <- learnErrors(filtRs, multithread=TRUE)

#save(errF, file="./intermediate/MBLand_errF.RData")
#save(errR, file="./intermediate/MBLand_errR.RData")
load("./intermediate/MBLand_errF.RData")
load("./intermediate/MBLand_errR.RData")
```

#### Plant Error Models
```{r, results='hide', echo=FALSE}
#sanity check, plot errors
#pdf("figures/MBLand-ErrorLearning.pdf", 
#     width=16/2.54, height=16/2.54)
plotErrors(errF, nominalQ=TRUE)  
```

-

```{r, results='hide', echo=FALSE}
plotErrors(errR, nominalQ=TRUE)
#dev.off()
```


### Dereplicate

```{r, results='hide'}
#dereplicate identical reads into unique reads (with an abundance/count value)
#derepFs <- derepFastq(filtFs, verbose=TRUE)
#derepRs <- derepFastq(filtRs, verbose=TRUE)

# Name the derep-class objects by the sample names
#names(derepFs) <- Sample.ID
#names(derepRs) <- Sample.ID
```
```{r, results='hide', echo=FALSE}
#save(derepFs, file="./intermediate/MBLand_derepFs.RData")
#save(derepRs, file="./intermediate/MBLand_derepRs.RData")

load("./intermediate/MBLand_derepFs.RData")
load("./intermediate/MBLand_derepRs.RData")
derepFs[1] #example
```

### Denoise

```{r, results='hide'}
# core denoising algorithm
#   is built on the parametric error model inferred directly from reads. 
#dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
#dadaRs <- dada(derepRs, err=errR, multithread=TRUE)

#save(dadaFs, file="./intermediate/MBLand_dadaFs.RData")
#save(dadaRs, file="./intermediate/MBLand_dadaRs.RData")
load("./intermediate/MBLand_dadaFs.RData")
load("./intermediate/MBLand_dadaRs.RData")
dadaFs[1] #example
```

# 5) Make Contigs

```{r, results='hide'}
#merge fwd and rev reads together, i.e. contigs     #2mins
#mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = TRUE) 
#additional arg. minOverlap

#save(mergers, file="./intermediate/MBLand_Mergers.RData")
load("./intermediate/MBLand_Mergers.RData")
```
```{r, results='hide', echo=FALSE}
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

# make amplicon sequence variant table (ASV) table
#seqtab <- makeSequenceTable(mergers)
#dim(seqtab)
#[1]   243 1642

#save(seqtab, file="./intermediate/MBLand_seqtab.RData") 
load("./intermediate/MBLand_seqtab.RData")
```

### Get a sense of contig length variation
```{r, results='hide', echo=FALSE}
#table(nchar(getSequences(seqtab))) #146-545bp
hist(nchar(getSequences(seqtab)), main = "Seq. Length")
med.seqtab <- median(nchar(getSequences(seqtab))); #med.seqtab #242bp
abline(v= med.seqtab, lty=2, col='red', lwd=3) 
```

* Median basepair length is 241.5

# 6) Chimera checking
```{r, results='hide'}
# identify chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
#Identified 5 bimeras out of 1642 input sequences.
```


### Freq. of chimeric sequences
```{r}
# plants
sum(seqtab.nochim)/sum(seqtab)
```

* <0.01% chimeras by adundance

```{r, results='hide', echo=FALSE}
#dim(seqtab.nochim)
#[1]  243 1637
#save(seqtab.nochim, file="./intermediate/MBLand_seqtab.nochim.RData")
#write.csv(seqtab.nochim, "./data/MBLand_SbyS.csv")

```

# 7) Track Reads

**(hidden)**

```{r, results='hide', echo=FALSE}
#subset again to remove any samples that did not pass filtering
out2 <- out[not.lost,]

# track reads through the pipeline
getN <- function(x) sum(getUniques(x))
#track <- cbind(SeqsOrig.df[,1], 
#              out2[,1], 
#              round(out2[,1]/SeqsOrig.df[,1]*100, 2),  
#              out2[,2], 
#              round(out2[,2]/out2[,1]*100,2), 
#              sapply(dadaFs, getN), sapply(dadaRs, getN), 
#              round(sapply(dadaFs, getN)/out2[,2]*100,2),
#              sapply(mergers, getN), 
#              round(sapply(mergers, getN)/sapply(dadaFs, getN)*100,2),
#              rowSums(seqtab.nochim), 
#              round(rowSums(seqtab.nochim)/sapply(mergers, getN)*100,2))
## If processing a single sample, remove the sapply calls: 
##   e.g. replace sapply(dadaFs, getN) with getN(dadaFs)

#colnames(track) <- c("OrigSeqsF", "post1stFilter", 
#  "PercKept1stFilter", "post2ndFilter", "PercKept2ndFilter","denoisedF", 
#  "denoisedR", "PercKeptDenoise", "Merged", "PercKeptMerge",
#  "postChimera","PercKeptChimera")
#rownames(track) <- Sample.ID
#head(track)

#write.csv(track, "./intermediate/MBLand_TrackSequences.csv")
track <- read.csv("./intermediate/MBLand_TrackSequences.csv", 
                 row.names = 1)
```


# 8) Organize post-DADA2 datasets

```{r, results='hide', echo=FALSE}
SbyS <- read.csv("./data/MBLand_SbyS.csv", row.names=1)
load("./intermediate/MBLand_seqtab.nochim.RData")
dim(seqtab.nochim); dim(SbyS)  
#243 1637
```

```{r, results='hide', echo=FALSE}
#SbyE <- read.csv("./data/MBLand_AllPlotCodes_2020_10_19.csv", row.names = 1)  ## ORIGINALLY USED
#SbyE <- droplevels(SbyE[SbyE$unique.plot %in% rownames(SbyS), ])  #drop samples not sequenced
#SbyE <- read.csv("./data/MBLand_SbyE_2021-04-27edited.csv", row.names = 1) #also formerly used


SbyE <- read.csv("./data/MBLand_SbyE_2021-10-04-2nd.csv", row.names = 1,
                 stringsAsFactors = TRUE)
rownames(SbyE) <- SbyE$unique.plot
str(SbyE)
# sanity check, must be true
identical(sort(rownames(SbyE)), sort(rownames(seqtab.nochim)))



# #phyloseq object
# ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE),
#                     sample_data(SbyE))
# dna <- Biostrings::DNAStringSet(taxa_names(ps))
# names(dna) <- taxa_names(ps)
# ps <- merge_phyloseq(ps, dna)
# taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
# ps
# #output my own fasta file
# writeXStringSet(refseq(ps),
#                 filepath = "./data/MBLand_uniqueSeqs.fasta")
# uniqueSS <- readDNAStringSet("./data/MBLand_uniqueSeqs.fasta")
# # save ps object
# save(ps, file="./data/MBLand_ps.RData")
load("./data/MBLand_ps.RData")
ps

```

* Number of unique ASVs: `r length(getUniques(seqtab.nochim))`
* Pre-nonFungal ASV removal and pre-LULU average read count `r round(mean(rowSums(SbyS)[1:239]),0)`











##### end